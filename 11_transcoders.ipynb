{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2025-nlp/blob/main/11_transcoders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52fb5831",
      "metadata": {
        "id": "52fb5831"
      },
      "source": [
        "# Transcodersによる解釈"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f53f27d",
      "metadata": {
        "id": "8f53f27d"
      },
      "source": [
        "* Transcodersとは・・・\n",
        "  * 元のモデルの中間層の表現を、より解釈しやすい表現に変換するためのニューラルネットワークである。\n",
        "* 関連する論文\n",
        "  * https://arxiv.org/abs/2406.11944\n",
        "  * https://arxiv.org/abs/2408.05147"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4749ab6",
      "metadata": {
        "id": "d4749ab6"
      },
      "source": [
        "* 今回使ってみるtranscoders\n",
        "  * https://huggingface.co/google/gemma-scope-2b-pt-transcoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6721cbac",
      "metadata": {
        "id": "6721cbac"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from huggingface_hub import hf_hub_download, login\n",
        "\n",
        "access_token = \"\" # ここには自分のアクセストークンを書き込む\n",
        "# あるいは、次のセルをaccess_tokenなしで実行して、そのつど入力してもよい。\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfb0b3f6",
      "metadata": {
        "id": "cfb0b3f6"
      },
      "outputs": [],
      "source": [
        "login(access_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "244c3fe4",
      "metadata": {
        "id": "244c3fe4"
      },
      "outputs": [],
      "source": [
        "torch.set_grad_enabled(False) # avoid blowing up mem\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"google/gemma-2-2b\",\n",
        "    device_map='auto',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7c566c5",
      "metadata": {
        "id": "c7c566c5"
      },
      "outputs": [],
      "source": [
        "tokenizer =  AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e6b35c4",
      "metadata": {
        "id": "5e6b35c4"
      },
      "outputs": [],
      "source": [
        "# The input text\n",
        "prompt = \"Would you be able to travel through time using a wormhole?\"\n",
        "\n",
        "# Use the tokenizer to convert it to tokens. Note that this implicitly adds a special \"Beginning of Sequence\" or <bos> token to the start\n",
        "inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(device)\n",
        "print(inputs)\n",
        "\n",
        "# Pass it in to the model and generate text\n",
        "outputs = model.generate(input_ids=inputs, max_new_tokens=2)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "154a807e",
      "metadata": {
        "id": "154a807e"
      },
      "source": [
        "* `l0_`の後の数値は、スパース性の度合いを表している。\n",
        "  * ざっくり言えば、反応するニューロンの個数の期待値。\n",
        "* この値が小さいtranscoderは、再構成の正確さを犠牲にして、より少数のニューロンしか発火しないようにしている。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cf2d208",
      "metadata": {
        "id": "3cf2d208"
      },
      "outputs": [],
      "source": [
        "path_to_params = hf_hub_download(\n",
        "    repo_id=\"google/gemma-scope-2b-pt-transcoders\",\n",
        "    filename=\"layer_20/width_16k/average_l0_11/params.npz\",\n",
        "    force_download=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cfd5d4d",
      "metadata": {
        "id": "8cfd5d4d"
      },
      "outputs": [],
      "source": [
        "params = np.load(path_to_params)\n",
        "pt_params = {k: torch.from_numpy(v).to(device) for k, v in params.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ece4778",
      "metadata": {
        "id": "6ece4778"
      },
      "outputs": [],
      "source": [
        "pt_params['threshold'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16225a6b",
      "metadata": {
        "id": "16225a6b"
      },
      "outputs": [],
      "source": [
        "pt_params[\"W_enc\"].norm(dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfa236fc",
      "metadata": {
        "id": "bfa236fc"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class JumpReLUSAE(nn.Module):\n",
        "  def __init__(self, d_model, d_sae):\n",
        "    # Note that we initialise these to zeros because we're loading in pre-trained weights.\n",
        "    # If you want to train your own SAEs then we recommend using blah\n",
        "    super().__init__()\n",
        "    self.W_enc = nn.Parameter(torch.zeros(d_model, d_sae))\n",
        "    self.W_dec = nn.Parameter(torch.zeros(d_sae, d_model))\n",
        "    self.threshold = nn.Parameter(torch.zeros(d_sae))\n",
        "    self.b_enc = nn.Parameter(torch.zeros(d_sae))\n",
        "    self.b_dec = nn.Parameter(torch.zeros(d_model))\n",
        "\n",
        "  def encode(self, input_acts):\n",
        "    pre_acts = input_acts @ self.W_enc + self.b_enc\n",
        "    mask = (pre_acts > self.threshold)\n",
        "    acts = mask * torch.nn.functional.relu(pre_acts)\n",
        "    return acts\n",
        "\n",
        "  def decode(self, acts):\n",
        "    return acts @ self.W_dec + self.b_dec\n",
        "\n",
        "  def forward(self, acts):\n",
        "    acts = self.encode(acts)\n",
        "    recon = self.decode(acts)\n",
        "    return recon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9155147",
      "metadata": {
        "id": "d9155147"
      },
      "outputs": [],
      "source": [
        "transcoder = JumpReLUSAE(params['W_enc'].shape[0], params['W_enc'].shape[1])\n",
        "transcoder.load_state_dict(pt_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de4d5d8f",
      "metadata": {
        "id": "de4d5d8f"
      },
      "outputs": [],
      "source": [
        "model.model.layers[20]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7a557b5",
      "metadata": {
        "id": "c7a557b5"
      },
      "outputs": [],
      "source": [
        "def gather_residual_activations(model, target_layer, inputs):\n",
        "    target_act = []\n",
        "    def gather_target_act_hook(mod, inputs, outputs):\n",
        "        nonlocal target_act # make sure we can modify the target_act from the outer scope\n",
        "        target_act.append((inputs[0].cpu(), outputs.cpu()))\n",
        "        return outputs\n",
        "    layer = model.model.layers[target_layer]\n",
        "    handle_pre = layer.pre_feedforward_layernorm.register_forward_hook(gather_target_act_hook)\n",
        "    handle_post = layer.post_feedforward_layernorm.register_forward_hook(gather_target_act_hook)\n",
        "    model.forward(inputs)\n",
        "    handle_pre.remove()\n",
        "    handle_post.remove()\n",
        "    return target_act"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "857bc31a",
      "metadata": {
        "id": "857bc31a"
      },
      "outputs": [],
      "source": [
        "target_act = gather_residual_activations(model, 20, inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6e79a22",
      "metadata": {
        "id": "d6e79a22"
      },
      "outputs": [],
      "source": [
        "target_act[0][0].shape, target_act[0][1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49534791",
      "metadata": {
        "id": "49534791"
      },
      "outputs": [],
      "source": [
        "target_act[1][0].shape, target_act[1][1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01a79ccd",
      "metadata": {
        "id": "01a79ccd"
      },
      "outputs": [],
      "source": [
        "target_act[0][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0fa111b",
      "metadata": {
        "id": "a0fa111b"
      },
      "outputs": [],
      "source": [
        "target_act[1][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5483c656",
      "metadata": {
        "id": "5483c656"
      },
      "outputs": [],
      "source": [
        "transcoder_acts = transcoder.encode(target_act[0][1].to(torch.float32))\n",
        "recon = transcoder.decode(transcoder_acts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e546c02",
      "metadata": {
        "id": "4e546c02"
      },
      "outputs": [],
      "source": [
        "recon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd0dac38",
      "metadata": {
        "id": "cd0dac38"
      },
      "outputs": [],
      "source": [
        "1 - torch.mean((recon[0, :, 1:] - target_act[1][1][0, :, 1:].to(torch.float32)) **2) / (target_act[1][1][0, :, 1:].to(torch.float32).var())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f874d250",
      "metadata": {
        "id": "f874d250"
      },
      "outputs": [],
      "source": [
        "(transcoder_acts > 1).sum(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff83507a",
      "metadata": {
        "id": "ff83507a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python (3.12.11)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}