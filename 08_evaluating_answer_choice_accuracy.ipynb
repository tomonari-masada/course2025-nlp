{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2025-nlp/blob/main/08_evaluating_answer_choice_accuracy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLMを使った多肢選択式質問への応答"
      ],
      "metadata": {
        "id": "9kNHx2WomSHN"
      },
      "id": "9kNHx2WomSHN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 今回は、とりあえず、生成モデルをちょっと使ってみる。"
      ],
      "metadata": {
        "id": "DcJmodLDmV6e"
      },
      "id": "DcJmodLDmV6e"
    },
    {
      "cell_type": "markdown",
      "id": "9082df75",
      "metadata": {
        "id": "9082df75"
      },
      "source": [
        "* 以下の記事を参考にした。\n",
        "  * https://magazine.sebastianraschka.com/p/llm-evaluation-4-approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ランタイムのタイプをGPUに変更しておこう。"
      ],
      "metadata": {
        "id": "s3QqIFSVmpNN"
      },
      "id": "s3QqIFSVmpNN"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## データセット"
      ],
      "metadata": {
        "id": "Qn_PFLvImaOl"
      },
      "id": "Qn_PFLvImaOl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* MMLUデータセットから、`high_school_mathematics`の部分を使う。\n",
        "  * https://huggingface.co/datasets/cais/mmlu"
      ],
      "metadata": {
        "id": "ZCudZajQmbgn"
      },
      "id": "ZCudZajQmbgn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c4cc9c4",
      "metadata": {
        "id": "0c4cc9c4"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"cais/mmlu\", \"high_school_mathematics\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa6cf89b",
      "metadata": {
        "id": "aa6cf89b"
      },
      "outputs": [],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 今回使用するのは`test`スライス\n",
        "  * 生成モデルをいきなりそのまま使うだけなので。"
      ],
      "metadata": {
        "id": "y_D7dMjFm16C"
      },
      "id": "y_D7dMjFm16C"
    },
    {
      "cell_type": "markdown",
      "id": "2a5d859b",
      "metadata": {
        "id": "2a5d859b"
      },
      "source": [
        "## プロンプト作成のためのヘルパ関数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fde4c234",
      "metadata": {
        "id": "fde4c234"
      },
      "outputs": [],
      "source": [
        "def format_prompt(example):\n",
        "    return (\n",
        "        f\"{example['question']}\\n\"\n",
        "        f\"A. {example['choices'][0]}\\n\"\n",
        "        f\"B. {example['choices'][1]}\\n\"\n",
        "        f\"C. {example['choices'][2]}\\n\"\n",
        "        f\"D. {example['choices'][3]}\\n\"\n",
        "        \"Answer: \"  # trailing space encourages a single-letter next token\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fabbfc06",
      "metadata": {
        "id": "fabbfc06"
      },
      "source": [
        "* プロンプト作成を試してみる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86468cbd",
      "metadata": {
        "id": "86468cbd"
      },
      "outputs": [],
      "source": [
        "ds[\"test\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 多肢選択問題になっている。"
      ],
      "metadata": {
        "id": "X2rTREIrnEE2"
      },
      "id": "X2rTREIrnEE2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32c55834",
      "metadata": {
        "id": "32c55834"
      },
      "outputs": [],
      "source": [
        "print(format_prompt(ds[\"test\"][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## モデルの取得"
      ],
      "metadata": {
        "id": "B1oZJgxInIXY"
      },
      "id": "B1oZJgxInIXY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 今回はQwenの軽量なLLMにしておく。\n",
        "  * https://huggingface.co/Qwen/Qwen3-0.6B"
      ],
      "metadata": {
        "id": "AdxZO9yWnJ41"
      },
      "id": "AdxZO9yWnJ41"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "518da419",
      "metadata": {
        "id": "518da419"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " * 推論するだけなので、evalutionモードに設定する。"
      ],
      "metadata": {
        "id": "7tvz-p6znWnT"
      },
      "id": "7tvz-p6znWnT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0638e398",
      "metadata": {
        "id": "0638e398"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* トークナイザを試してみる。"
      ],
      "metadata": {
        "id": "HoOU2M6tniAh"
      },
      "id": "HoOU2M6tniAh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "561a0de1",
      "metadata": {
        "id": "561a0de1"
      },
      "outputs": [],
      "source": [
        "prompt = format_prompt(ds[\"test\"][0])\n",
        "tok = torch.tensor(tokenizer.encode(prompt), device=device).unsqueeze(0)\n",
        "tok"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* どのようなサブワードに分割されているかを見てみる。"
      ],
      "metadata": {
        "id": "9CBmSvibnlAE"
      },
      "id": "9CBmSvibnlAE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ee03895",
      "metadata": {
        "id": "1ee03895"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.convert_ids_to_tokens(tok[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 答えの生成"
      ],
      "metadata": {
        "id": "8bNi9Hn2npyy"
      },
      "id": "8bNi9Hn2npyy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c314bfca",
      "metadata": {
        "id": "c314bfca"
      },
      "outputs": [],
      "source": [
        "out = model.generate(tok, max_new_tokens=8, temperature=0.0, do_sample=False)\n",
        "print(tokenizer.decode(out.squeeze(0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* attention maskをちゃんと渡すには以下のようにする。"
      ],
      "metadata": {
        "id": "9wnVgn9noxNO"
      },
      "id": "9wnVgn9noxNO"
    },
    {
      "cell_type": "code",
      "source": [
        "input = tokenizer(prompt, return_tensors=\"pt\")\n",
        "tok = input['input_ids'].to(device)\n",
        "mask = input['attention_mask'].to(device)"
      ],
      "metadata": {
        "id": "w-1EvmXBoL-v"
      },
      "id": "w-1EvmXBoL-v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = model.generate(tok, attention_mask=mask, max_new_tokens=8, temperature=0.0, do_sample=False)\n",
        "print(tokenizer.decode(out.squeeze(0)))"
      ],
      "metadata": {
        "id": "lVCpXQ_Tn23W"
      },
      "id": "lVCpXQ_Tn23W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5211f423",
      "metadata": {
        "id": "5211f423"
      },
      "outputs": [],
      "source": [
        "answer = tokenizer.decode(out.squeeze(0)[len(tok[0]):], skip_special_tokens=True)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25e53ec4",
      "metadata": {
        "id": "25e53ec4"
      },
      "outputs": [],
      "source": [
        "pred = None\n",
        "for letter in answer:\n",
        "    letter = letter.upper()\n",
        "    if letter in \"ABCD\":\n",
        "        pred = letter\n",
        "        break\n",
        "if pred is None:\n",
        "    pred = \"N/A\"\n",
        "\n",
        "ans = ds[\"test\"][0][\"answer\"]\n",
        "gold = \"ABCD\"[ans] if isinstance(ans, int) else str(ans).strip().upper()\n",
        "\n",
        "print(f\"Predicted: {pred}, Correct: {gold}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aa262fe",
      "metadata": {
        "id": "0aa262fe"
      },
      "source": [
        "* ここまでの処理をヘルパ関数としてまとめる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1deee7a0",
      "metadata": {
        "id": "1deee7a0"
      },
      "outputs": [],
      "source": [
        "def predict_choice(example):\n",
        "    prompt = format_prompt(example)\n",
        "\n",
        "    input = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    tok = input['input_ids'].to(device)\n",
        "    mask = input['attention_mask'].to(device)\n",
        "\n",
        "    out = model.generate(tok, attention_mask=mask, max_new_tokens=8, temperature=0.0, do_sample=False)\n",
        "    answer = tokenizer.decode(out.squeeze(0)[len(tok[0]):], skip_special_tokens=True)\n",
        "\n",
        "    pred = None\n",
        "    for letter in answer:\n",
        "        letter = letter.upper()\n",
        "        if letter in \"ABCD\":\n",
        "            pred = letter\n",
        "            break\n",
        "    if pred is None:\n",
        "        pred = \"N/A\"\n",
        "\n",
        "    ans = example[\"answer\"]\n",
        "    gold = \"ABCD\"[ans] if isinstance(ans, int) else str(ans).strip().upper()\n",
        "\n",
        "    return pred, gold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b47e4f69",
      "metadata": {
        "id": "b47e4f69"
      },
      "outputs": [],
      "source": [
        "predict_choice(ds[\"test\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 評価"
      ],
      "metadata": {
        "id": "Zde6UcEXpQa5"
      },
      "id": "Zde6UcEXpQa5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* どのくらい正答するか、少し様子を見てみる。"
      ],
      "metadata": {
        "id": "_7wjV27ipRjh"
      },
      "id": "_7wjV27ipRjh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd8cd168",
      "metadata": {
        "id": "cd8cd168"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    pred, gold = predict_choice(ds[\"test\"][i])\n",
        "    print(f\"Q{i+1}: Predicted: {pred}, Correct: {gold}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* `test`スライス全体で正解率を求める。"
      ],
      "metadata": {
        "id": "aJecWQiypVZm"
      },
      "id": "aJecWQiypVZm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4453a91a",
      "metadata": {
        "id": "4453a91a"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "total = 0\n",
        "correct = 0\n",
        "start = time.time()\n",
        "for i in range(len(ds[\"test\"])):\n",
        "    pred, gold = predict_choice(ds[\"test\"][i])\n",
        "    total += 1\n",
        "    if pred == gold:\n",
        "        correct += 1\n",
        "    if (i + 1) % 10 == 0:\n",
        "        end = time.time()\n",
        "        print(f\"Processed {i+1}/{len(ds['test'])} in {end - start:.1f} sec\")\n",
        "        start = end\n",
        "        print(f\"  Current accuracy: {correct}/{total} = {correct/total:.3%}\")\n",
        "end = time.time()\n",
        "print(f\"Accuracy: {correct}/{total} = {correct/total:.3%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a732ab64",
      "metadata": {
        "id": "a732ab64"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}