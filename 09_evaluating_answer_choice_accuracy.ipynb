{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2025-nlp/blob/main/09_evaluating_answer_choice_accuracy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9kNHx2WomSHN",
      "metadata": {
        "id": "9kNHx2WomSHN"
      },
      "source": [
        "# LLMを使った多肢選択式質問への応答"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DcJmodLDmV6e",
      "metadata": {
        "id": "DcJmodLDmV6e"
      },
      "source": [
        "* 今回は、とりあえず、生成モデルをちょっと使ってみる。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9082df75",
      "metadata": {
        "id": "9082df75"
      },
      "source": [
        "* 以下の記事を参考にした。\n",
        "  * https://magazine.sebastianraschka.com/p/llm-evaluation-4-approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s3QqIFSVmpNN",
      "metadata": {
        "id": "s3QqIFSVmpNN"
      },
      "source": [
        "* ランタイムのタイプをGPUに変更しておこう。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "041c3319",
      "metadata": {
        "id": "041c3319"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Qn_PFLvImaOl",
      "metadata": {
        "id": "Qn_PFLvImaOl"
      },
      "source": [
        "## データセット"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZCudZajQmbgn",
      "metadata": {
        "id": "ZCudZajQmbgn"
      },
      "source": [
        "* MMLUデータセットから、`high_school_mathematics`の部分を使う。\n",
        "  * https://huggingface.co/datasets/cais/mmlu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c4cc9c4",
      "metadata": {
        "id": "0c4cc9c4"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"cais/mmlu\", \"high_school_mathematics\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa6cf89b",
      "metadata": {
        "id": "aa6cf89b"
      },
      "outputs": [],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "y_D7dMjFm16C",
      "metadata": {
        "id": "y_D7dMjFm16C"
      },
      "source": [
        "* 今回使用するのは`test`スライス\n",
        "  * 生成モデルをいきなりそのまま使うだけなので。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a5d859b",
      "metadata": {
        "id": "2a5d859b"
      },
      "source": [
        "## プロンプト作成のためのヘルパ関数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fde4c234",
      "metadata": {
        "id": "fde4c234"
      },
      "outputs": [],
      "source": [
        "def format_prompt(example):\n",
        "    return (\n",
        "        f\"{example['question']}\\n\"\n",
        "        f\"A. {example['choices'][0]}\\n\"\n",
        "        f\"B. {example['choices'][1]}\\n\"\n",
        "        f\"C. {example['choices'][2]}\\n\"\n",
        "        f\"D. {example['choices'][3]}\\n\"\n",
        "        \"Answer: \"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fabbfc06",
      "metadata": {
        "id": "fabbfc06"
      },
      "source": [
        "* プロンプト作成を試してみる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86468cbd",
      "metadata": {
        "id": "86468cbd"
      },
      "outputs": [],
      "source": [
        "ds[\"test\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "X2rTREIrnEE2",
      "metadata": {
        "id": "X2rTREIrnEE2"
      },
      "source": [
        "* 多肢選択問題になっている。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32c55834",
      "metadata": {
        "id": "32c55834"
      },
      "outputs": [],
      "source": [
        "print(format_prompt(ds[\"test\"][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B1oZJgxInIXY",
      "metadata": {
        "id": "B1oZJgxInIXY"
      },
      "source": [
        "## モデルの取得"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AdxZO9yWnJ41",
      "metadata": {
        "id": "AdxZO9yWnJ41"
      },
      "source": [
        "* 今回はQwenの軽量なLLMにしておく。\n",
        "  * https://huggingface.co/Qwen/Qwen3-0.6B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "518da419",
      "metadata": {
        "id": "518da419"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name = \"Qwen/Qwen3-0.6B\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, dtype=\"auto\", device_map=\"auto\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HoOU2M6tniAh",
      "metadata": {
        "id": "HoOU2M6tniAh"
      },
      "source": [
        "* トークナイザを試してみる。\n",
        "  * トークナイザの`apply_chat_template`は、あえて使っていない。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "561a0de1",
      "metadata": {
        "id": "561a0de1"
      },
      "outputs": [],
      "source": [
        "prompt = format_prompt(ds[\"test\"][0])\n",
        "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "model_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9CBmSvibnlAE",
      "metadata": {
        "id": "9CBmSvibnlAE"
      },
      "source": [
        "* どのようなサブワードに分割されているかを見てみる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ee03895",
      "metadata": {
        "id": "1ee03895"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.convert_ids_to_tokens(model_inputs[\"input_ids\"][0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bNi9Hn2npyy",
      "metadata": {
        "id": "8bNi9Hn2npyy"
      },
      "source": [
        "## 答えの生成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c314bfca",
      "metadata": {
        "id": "c314bfca"
      },
      "outputs": [],
      "source": [
        "out = model.generate(**model_inputs, max_new_tokens=8, temperature=0.0, do_sample=False)\n",
        "print(tokenizer.decode(out.squeeze(0)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7258ceac",
      "metadata": {
        "id": "7258ceac"
      },
      "outputs": [],
      "source": [
        "answer = tokenizer.decode(out.squeeze(0)[len(model_inputs[\"input_ids\"][0]):], skip_special_tokens=True)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e18f56ab",
      "metadata": {
        "id": "e18f56ab"
      },
      "source": [
        "* 素朴に答えの記号が入っているかどうかのチェックだけしている。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25e53ec4",
      "metadata": {
        "id": "25e53ec4"
      },
      "outputs": [],
      "source": [
        "pred = None\n",
        "for letter in answer:\n",
        "    letter = letter.upper()\n",
        "    if letter in \"ABCD\":\n",
        "        pred = letter\n",
        "        break\n",
        "if pred is None:\n",
        "    pred = \"N/A\"\n",
        "\n",
        "ans = ds[\"test\"][0][\"answer\"]\n",
        "gold = \"ABCD\"[ans] if isinstance(ans, int) else str(ans).strip().upper()\n",
        "\n",
        "print(f\"Predicted: {pred}, Correct: {gold}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0aa262fe",
      "metadata": {
        "id": "0aa262fe"
      },
      "source": [
        "* ここまでの処理をヘルパ関数としてまとめる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1deee7a0",
      "metadata": {
        "id": "1deee7a0"
      },
      "outputs": [],
      "source": [
        "def predict_choice(example):\n",
        "    prompt = format_prompt(example)\n",
        "\n",
        "    model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    out = model.generate(**model_inputs, max_new_tokens=8, temperature=0.0, do_sample=False)\n",
        "    answer = tokenizer.decode(out.squeeze(0)[len(model_inputs[\"input_ids\"][0]):], skip_special_tokens=True)\n",
        "\n",
        "    pred = None\n",
        "    for letter in answer:\n",
        "        letter = letter.upper()\n",
        "        if letter in \"ABCD\":\n",
        "            pred = letter\n",
        "            break\n",
        "    if pred is None:\n",
        "        pred = \"N/A\"\n",
        "\n",
        "    ans = example[\"answer\"]\n",
        "    gold = \"ABCD\"[ans] if isinstance(ans, int) else str(ans).strip().upper()\n",
        "\n",
        "    return pred, gold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b47e4f69",
      "metadata": {
        "id": "b47e4f69"
      },
      "outputs": [],
      "source": [
        "predict_choice(ds[\"test\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Zde6UcEXpQa5",
      "metadata": {
        "id": "Zde6UcEXpQa5"
      },
      "source": [
        "## 評価"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_7wjV27ipRjh",
      "metadata": {
        "id": "_7wjV27ipRjh"
      },
      "source": [
        "* どのくらい正答するか、少し様子を見てみる。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd8cd168",
      "metadata": {
        "id": "cd8cd168"
      },
      "outputs": [],
      "source": [
        "for i in range(10):\n",
        "    pred, gold = predict_choice(ds[\"test\"][i])\n",
        "    print(f\"Q{i+1}: Predicted: {pred}, Correct: {gold}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aJecWQiypVZm",
      "metadata": {
        "id": "aJecWQiypVZm"
      },
      "source": [
        "* `test`スライス全体で正解率を求める。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4453a91a",
      "metadata": {
        "id": "4453a91a"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "total = 0\n",
        "correct = 0\n",
        "start = time.time()\n",
        "for i in range(len(ds[\"test\"])):\n",
        "    pred, gold = predict_choice(ds[\"test\"][i])\n",
        "    total += 1\n",
        "    if pred == gold:\n",
        "        correct += 1\n",
        "    if (i + 1) % 10 == 0:\n",
        "        end = time.time()\n",
        "        print(f\"Processed {i+1}/{len(ds['test'])} in {end - start:.1f} sec\")\n",
        "        start = end\n",
        "        print(f\"  Current accuracy: {correct}/{total} = {correct/total:.3%}\")\n",
        "end = time.time()\n",
        "print(f\"Accuracy: {correct}/{total} = {correct/total:.3%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a732ab64",
      "metadata": {
        "id": "a732ab64"
      },
      "outputs": [],
      "source": [
        "def format_prompt(example):\n",
        "    QA_test = (\n",
        "         \"You are a highly intelligent question answering bot. \"\n",
        "         \"If you don't know the answer, just say 'N/A'. \"\n",
        "         \"Do not attempt to fabricate an answer.\\n\\n\"\n",
        "    )\n",
        "    prompt = (\n",
        "        QA_test +\n",
        "        f\"{example['question']}\\n\"\n",
        "        f\"A. {example['choices'][0]}\\n\"\n",
        "        f\"B. {example['choices'][1]}\\n\"\n",
        "        f\"C. {example['choices'][2]}\\n\"\n",
        "        f\"D. {example['choices'][3]}\\n\"\n",
        "        \"Answer: \"\n",
        "    )\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": prompt}\n",
        "    ]\n",
        "    return messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c5f6e18",
      "metadata": {
        "id": "6c5f6e18"
      },
      "outputs": [],
      "source": [
        "messages = format_prompt(ds[\"test\"][0])\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        "    enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.\n",
        ")\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# conduct text completion\n",
        "generated_ids = model.generate(\n",
        "    **model_inputs,\n",
        "    max_new_tokens=32768\n",
        ")\n",
        "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dfe31cc",
      "metadata": {
        "id": "0dfe31cc"
      },
      "outputs": [],
      "source": [
        "answer = tokenizer.decode(output_ids, skip_special_tokens=True)\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4430fa2d",
      "metadata": {
        "id": "4430fa2d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}