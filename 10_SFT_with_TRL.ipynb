{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2025-nlp/blob/main/10_SFT_with_TRL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLD7NOGLE-ZZ"
      },
      "source": [
        "* ‰ª•‰∏ã„ÅÆ„Ç≥„Éº„Éâ„ÅØ„ÄÅLLM„ÅÆ„É¢„Éá„É´„Ç´„Éº„Éâ„Åã„Çâ„É™„É≥„ÇØ„ÅåË≤º„Çâ„Çå„Å¶„ÅÑ„Çãnotebook„ÇíÂ∞ë„ÅóÂ§âÊõ¥„Åó„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ\n",
        "  * https://huggingface.co/LiquidAI/LFM2.5-1.2B-JP#%F0%9F%94%A7-fine-tuning\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3PTFH-H9Ozk"
      },
      "source": [
        "# üíß LFM2 - SFT with TRL\n",
        "\n",
        "This tutorial demonstrates how to fine-tune our LFM2 models, e.g. [`LiquidAI/LFM2.5-1.2B-Instruct`](https://huggingface.co/LiquidAI/LFM2.5-1.2B-Instruct), using the TRL library.\n",
        "\n",
        "Follow along if it's your first time using trl, or take single code snippets for your own workflow\n",
        "\n",
        "## üéØ What you'll find:\n",
        "- **SFT** (Supervised Fine-Tuning) - Basic instruction following\n",
        "- **LoRA + SFT** - Using LoRA (from PEFT) to SFT while on constrained hardware\n",
        "\n",
        "## üìã Prerequisites:\n",
        "- **GPU Runtime**: Select GPU in `Runtime` ‚Üí `Change runtime type`\n",
        "- **Hugging Face Account**: For accessing models and datasets\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0RPLu2h9ome"
      },
      "source": [
        "# üì¶ Installation & Setup\n",
        "\n",
        "First, let's install all the required packages:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FIcp_wo9nsR"
      },
      "outputs": [],
      "source": [
        "!pip install transformers>=4.54.0 trl>=0.18.2 peft>=0.15.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41UEf1uxCd6m"
      },
      "source": [
        "Let's now verify the packages are installed correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSJgYtHT_Os4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import transformers\n",
        "import trl\n",
        "import os\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "transformers.set_seed(42)\n",
        "\n",
        "print(f\"üì¶ PyTorch version: {torch.__version__}\")\n",
        "print(f\"ü§ó Transformers version: {transformers.__version__}\")\n",
        "print(f\"üìä TRL version: {trl.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_uXLzxQ_rnK"
      },
      "source": [
        "# Loading the model from Transformers ü§ó\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Dvk20o9E-Zf"
      },
      "source": [
        "* „É¢„Éá„É´„ÅØ„ÄÅ[`LiquidAI/LFM2.5-1.2B-JP`](https://huggingface.co/LiquidAI/LFM2.5-1.2B-JP)„Çí‰Ωø„ÅÑ„Åæ„Åô„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iA3erKM4-HhS"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_id = \"LiquidAI/LFM2.5-1.2B-JP\"\n",
        "\n",
        "print(\"üìö Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "print(\"üß† Loading model...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    dtype=\"bfloat16\",\n",
        "#   attn_implementation=\"flash_attention_2\" <- uncomment on compatible GPU\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Local model loaded successfully!\")\n",
        "print(f\"üî¢ Parameters: {model.num_parameters():,}\")\n",
        "print(f\"üìñ Vocab size: {len(tokenizer)}\")\n",
        "print(f\"üíæ Model size: ~{model.num_parameters() * 2 / 1e9:.1f} GB (bfloat16)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ABA6Yrm_lql"
      },
      "source": [
        "# üéØ Part 1: Supervised Fine-Tuning (SFT)\n",
        "\n",
        "SFT teaches the model to follow instructions by training on input-output pairs (instruction vs response). This is the foundation for creating instruction-following models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KufdgeypHtst"
      },
      "source": [
        "## Load an SFT Dataset\n",
        "\n",
        "We will use [HuggingFaceTB/smoltalk](https://huggingface.co/datasets/HuggingFaceTB/smoltalk), limiting ourselves to the first 5k samples for brevity. Feel free to change the limit by changing the slicing index in the parameter `split`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QAV1-uRE-Zg"
      },
      "source": [
        "* „Åì„Åì„Åß„ÅØ„ÄÅ‰ª•‰∏ã„ÅÆQitta„ÅÆË®ò‰∫ã„ÇíÂèÇËÄÉ„Å´„Åó„Å¶„ÄÅÂà•„ÅÆ„Éá„Éº„Çø„Çª„ÉÉ„Éà„Çí‰Ωø„ÅÑ„Åæ„Åô„ÄÇ\n",
        "  * https://qiita.com/t-hashiguchi/items/9f3b394ca0ae1c7e4d02"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEYqJgtOE-Zg"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "print(\"üì• Loading SFT dataset...\")\n",
        "ds = load_dataset(\"bbz662bbz/databricks-dolly-15k-ja-gozaru\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARgICwYUE-Zh"
      },
      "outputs": [],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3xITvRZE-Zh"
      },
      "outputs": [],
      "source": [
        "ds[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROzZcvKJE-Zh"
      },
      "source": [
        "* ÂèÇËÄÉ„Å´„Åó„Åünotebook„Åß‰Ωø„Çè„Çå„Å¶„ÅÑ„Çã`HuggingFaceTB/smoltalk`„Å®Âêå„Åò„Éï„Ç©„Éº„Éû„ÉÉ„Éà„Å´Â§âÊèõ„Åô„Çã„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7IePwidE-Zh"
      },
      "outputs": [],
      "source": [
        "def smoltalk_prompt_template(example, question_only=False):\n",
        "    if question_only:\n",
        "        return { \"content\": example[\"instruction\"] + example[\"input\"], \"role\": \"user\" }\n",
        "    else:\n",
        "        if example[\"input\"]:\n",
        "            return [\n",
        "                { \"content\": example[\"instruction\"] + example[\"input\"], \"role\": \"user\" },\n",
        "                { \"content\": example[\"output\"], \"role\": \"assistant\" }\n",
        "            ]\n",
        "        else:\n",
        "            return [\n",
        "                { \"content\": example[\"instruction\"], \"role\": \"user\" },\n",
        "                { \"content\": example[\"output\"], \"role\": \"assistant\" }\n",
        "            ]\n",
        "\n",
        "def add_messages(example):\n",
        "    example[\"messages\"] = smoltalk_prompt_template(example)\n",
        "    return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7bJunmLE-Zh"
      },
      "outputs": [],
      "source": [
        "ds = ds.map(add_messages, remove_columns=ds[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCe8O06-_Cps"
      },
      "outputs": [],
      "source": [
        "ds = ds[\"train\"].train_test_split(test_size=0.2)\n",
        "ds[\"validation\"] = ds[\"test\"].train_test_split(test_size=0.5)[\"test\"]\n",
        "ds[\"test\"] = ds[\"test\"].train_test_split(test_size=0.5)[\"train\"]\n",
        "\n",
        "train_dataset_sft = ds[\"train\"]\n",
        "eval_dataset_sft = ds[\"validation\"]\n",
        "\n",
        "print(\"‚úÖ SFT Dataset loaded:\")\n",
        "print(f\"   üìö Train samples: {len(train_dataset_sft)}\")\n",
        "print(f\"   üß™ Eval samples: {len(eval_dataset_sft)}\")\n",
        "print(f\"\\nüìù Single Sample: {train_dataset_sft[0]['messages']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5pI5JWpIlFQ"
      },
      "source": [
        "## Launch Training\n",
        "\n",
        "We are now ready to launch an SFT run with `SFTTrainer`, feel free to modify `SFTConfig` to play around with different configurations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Google ColabÁÑ°ÊñôÁâà„Å†„Å®„ÄÅGPU„ÅÆ„É°„É¢„É™„ÅåË∂≥„Çä„Å™„Åè„Å™„Çä„Åæ„Åô„ÄÇ"
      ],
      "metadata": {
        "id": "iAz2fAXkHCPN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixD8Po-eAbPp"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=\"./lfm2-sft\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    warmup_steps=100,\n",
        "    warmup_ratio=0.2,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=1000,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=None,\n",
        "    bf16=False # <- not all colab GPUs support bf16\n",
        ")\n",
        "\n",
        "print(\"üèóÔ∏è  Creating SFT trainer...\")\n",
        "sft_trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=sft_config,\n",
        "    train_dataset=train_dataset_sft,\n",
        "    eval_dataset=eval_dataset_sft,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"\\nüöÄ Starting SFT training...\")\n",
        "sft_trainer.train()\n",
        "\n",
        "print(\"üéâ SFT training completed!\")\n",
        "\n",
        "sft_trainer.save_model()\n",
        "print(f\"üíæ SFT model saved to: {sft_config.output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUvcudTFE-Zh"
      },
      "source": [
        "* „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Å´„Å§„ÅÑ„Å¶„ÄÅÁ≠î„Åà„ÇíÁîüÊàê„Åï„Åõ„Å¶„Åø„Åæ„Åô„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0CnHHeAE-Zh"
      },
      "outputs": [],
      "source": [
        "ds[\"test\"][0][\"messages\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4I7-uCkE-Zi"
      },
      "outputs": [],
      "source": [
        "ds[\"test\"][0][\"messages\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSlxv1ifE-Zi"
      },
      "source": [
        "* „ÉÜ„Ç≠„Çπ„ÉàÁîüÊàê„ÅÆÊñπÊ≥ï„ÅØ„É¢„Éá„É´„Ç´„Éº„Éâ„Å´„ÅÇ„ÇãÈÄö„Çä„ÄÇ\n",
        "  * https://huggingface.co/LiquidAI/LFM2.5-1.2B-JP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8dH0EHRE-Zi"
      },
      "outputs": [],
      "source": [
        "streamer = transformers.TextStreamer(tokenizer, skip_special_tokens=True)\n",
        "\n",
        "for i in range(3):\n",
        "    example = ds[\"test\"][i]\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        example[\"messages\"][0:-1],\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "        tokenize=True,\n",
        "    ).to(model.device)\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=True,\n",
        "        temperature=0.3,\n",
        "        min_p=0.15,\n",
        "        repetition_penalty=1.05,\n",
        "        max_new_tokens=256,\n",
        "        streamer=streamer,\n",
        "    )\n",
        "    print(f\"Ê≠£Ëß£‰æã: {example['messages'][-1]['content']}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08Y3TxKrBRXo"
      },
      "source": [
        "# üéõÔ∏è Part 2: LoRA + SFT (Parameter-Efficient Fine-tuning)\n",
        "\n",
        "LoRA (Low-Rank Adaptation) allows efficient fine-tuning by only training a small number of additional parameters. Perfect for limited compute resources!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MfWfc-Pvl9q"
      },
      "source": [
        "## Wrap the model with PEFT\n",
        "\n",
        "We specify target modules that will be finetuned while the rest of the models weights remains frozen. Feel free to modify the `r` (rank) value:\n",
        "- higher -> better approximation of full-finetuning\n",
        "- lower -> needs even less compute resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puYp_gTpBSsf"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "GLU_MODULES = [\"w1\", \"w2\", \"w3\"]\n",
        "MHA_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]\n",
        "CONV_MODULES = [\"in_proj\", \"out_proj\"]\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=8,  # <- lower values = fewer parameters\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=GLU_MODULES + MHA_MODULES + CONV_MODULES,\n",
        "    bias=\"none\",\n",
        "    modules_to_save=None,\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(model, lora_config)\n",
        "lora_model.print_trainable_parameters()\n",
        "\n",
        "print(\"‚úÖ LoRA configuration applied!\")\n",
        "print(f\"üéõÔ∏è  LoRA rank: {lora_config.r}\")\n",
        "print(f\"üìä LoRA alpha: {lora_config.lora_alpha}\")\n",
        "print(f\"üéØ Target modules: {lora_config.target_modules}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1Hem_DOwHgY"
      },
      "source": [
        "## Launch Training\n",
        "\n",
        "Now ready to launch the SFT training, but this time with the LoRA-wrapped model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-VYQysHBY8-"
      },
      "outputs": [],
      "source": [
        "from trl import SFTConfig, SFTTrainer\n",
        "\n",
        "lora_sft_config = SFTConfig(\n",
        "    output_dir=\"./lfm2-sft-lora\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=1,\n",
        "    learning_rate=5e-5,\n",
        "    lr_scheduler_type=\"linear\",\n",
        "    warmup_steps=100,\n",
        "    warmup_ratio=0.2,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=1000,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=None,\n",
        ")\n",
        "\n",
        "print(\"üèóÔ∏è  Creating LoRA SFT trainer...\")\n",
        "lora_sft_trainer = SFTTrainer(\n",
        "    model=lora_model,\n",
        "    args=lora_sft_config,\n",
        "    train_dataset=train_dataset_sft,\n",
        "    eval_dataset=eval_dataset_sft,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "print(\"\\nüöÄ Starting LoRA + SFT training...\")\n",
        "lora_sft_trainer.train()\n",
        "\n",
        "print(\"üéâ LoRA + SFT training completed!\")\n",
        "\n",
        "lora_sft_trainer.save_model()\n",
        "print(f\"üíæ LoRA model saved to: {lora_sft_config.output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI1-N-_Ev0cC"
      },
      "source": [
        "## Save merged model\n",
        "\n",
        "Merge the extra weights learned with LoRA back into the model to obtain a \"normal\" model checkpoint."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Google ColabÁÑ°ÊñôÁâà„Å†„Å®„ÄÅ„Åì„Åì„ÅåÈùûÂ∏∏„Å´ÈÅÖ„ÅÑ„Åß„Åô„Éª„Éª„Éª„ÄÇ"
      ],
      "metadata": {
        "id": "RHlIDN3gGuNZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rizEFUsvwce"
      },
      "outputs": [],
      "source": [
        "print(\"\\nüîÑ Merging LoRA weights...\")\n",
        "merged_model = lora_model.merge_and_unload()\n",
        "merged_model.save_pretrained(\"./lfm2-lora-merged\")\n",
        "tokenizer.save_pretrained(\"./lfm2-lora-merged\")\n",
        "print(\"üíæ Merged model saved to: ./lfm2-lora-merged\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dv3pKX5TE-Zi"
      },
      "source": [
        "* „É¢„Éá„É´„ÅÆ„É≠„Éº„Éâ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMdFMFwmE-Zj"
      },
      "outputs": [],
      "source": [
        "# load merged model for inference\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"./lfm2-lora-merged\",\n",
        "    device_map=\"auto\",\n",
        "    dtype=\"bfloat16\",\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"./lfm2-lora-merged\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* „ÉÜ„Çπ„Éà„Çª„ÉÉ„Éà„Å´„Å§„ÅÑ„Å¶„ÄÅÁ≠î„Åà„ÇíÁîüÊàê„Åï„Åõ„Å¶„Åø„Åæ„Åô„ÄÇ"
      ],
      "metadata": {
        "id": "P90w9BN3Hjxq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30ew-CvlE-Zj"
      },
      "outputs": [],
      "source": [
        "streamer = transformers.TextStreamer(tokenizer, skip_special_tokens=True)\n",
        "\n",
        "for i in range(3):\n",
        "    example = ds[\"test\"][i]\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        example[\"messages\"][0:-1],\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "        tokenize=True,\n",
        "    ).to(model.device)\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=True,\n",
        "        temperature=0.3,\n",
        "        min_p=0.15,\n",
        "        repetition_penalty=1.05,\n",
        "        max_new_tokens=256,\n",
        "        streamer=streamer,\n",
        "    )\n",
        "    print(f\"Ê≠£Ëß£‰æã: {example['messages'][-1]['content']}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxRKtjJME-Zj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}