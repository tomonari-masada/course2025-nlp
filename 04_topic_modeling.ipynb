{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomonari-masada/course2025-nlp/blob/main/04_topic_modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmIZbjYUNv9m"
      },
      "source": [
        "# トピックモデル\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* bag-of-wordsの範囲内でテキストデータの高度な分析を行う。\n",
        "* 教師なしでテキスト集合（＝コーパス）に含まれる多様なトピックを抽出する。\n",
        "* 今回は、潜在的ディリクレ配分法 (LDA; latent Dirichlet allocation) を使う。\n",
        "* scikit-learnにある実装を使う。\n",
        "  * https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html"
      ],
      "metadata": {
        "id": "HhY-JIvB6Mk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* コメント\n",
        "  * 現在であれば、テキスト埋め込み用のモデル\n",
        "を使ってベクトルに変換し・・・\n",
        "  * scikit-learnの適当なクラスタリング手法でクラスタリングする方が、\n",
        "  * 綺麗に多様なトピックを抽出できるかもしれない。"
      ],
      "metadata": {
        "id": "JPMRH86E6dJe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMHFkESgNv9o"
      },
      "source": [
        "**以下に示すようなチューニングをしてはじめて、LDAがその能力を発揮してくれます。**\n",
        "\n",
        "**デフォルトの設定のままでは十分な性能が出ません。**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 準備"
      ],
      "metadata": {
        "id": "Sd6AmasQe8JB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* pyLDAvisというLDAの可視化ツールをインストールする。\n",
        "  * セッションの再起動が必要かも。"
      ],
      "metadata": {
        "id": "NZaMrvnB9cbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyLDAvis"
      ],
      "metadata": {
        "id": "8I08XJJle3zL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh01qzsPNv9o"
      },
      "source": [
        "## データセット\n",
        "* Hugging Faceにある`CShorten/ML-ArXiv-Papers`を使う。\n",
        "  * https://huggingface.co/datasets/CShorten/ML-ArXiv-Papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evODt8mPNv9p"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"CShorten/ML-ArXiv-Papers\")\n",
        "ds = ds[\"train\"].train_test_split(test_size=0.1, seed=1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCHjvSJfNv9q"
      },
      "outputs": [],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 今回はタイトルを分析する。"
      ],
      "metadata": {
        "id": "enlljgKofRr7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-af7gyGBNv9q"
      },
      "outputs": [],
      "source": [
        "ds[\"train\"][\"title\"][:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXXmP9yGNv9q"
      },
      "source": [
        "## 単語の出現回数を数える"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* LDAを使うにはTF (term frequency) が必要。"
      ],
      "metadata": {
        "id": "UjvtCNAafYyG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSPu7c7zNv9q"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words=\"english\", min_df=20, max_df=0.5)\n",
        "X_train = vectorizer.fit_transform(ds[\"train\"][\"title\"])\n",
        "X_test = vectorizer.transform(ds[\"test\"][\"title\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdIbeVv3Nv9r"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1L7q1fLNv9r"
      },
      "outputs": [],
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMmKRrVTNv9r"
      },
      "source": [
        "## LDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJrUQQWyNv9r"
      },
      "source": [
        "* とりあえずLDAの変分推論を動かしてみる。\n",
        "  * 変分推論 (variational inference) については毎年「統計モデリング２」で説明しています。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpTfGYefNv9r"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "\n",
        "lda = LatentDirichletAllocation(\n",
        "  n_components=20,\n",
        "  evaluate_every=1,\n",
        "  verbose=1,\n",
        "  random_state=123,\n",
        ")\n",
        "lda.fit(X_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* training setとtest setでperplexityの差が大きい場合、学習がうまくいっていないことが多い。"
      ],
      "metadata": {
        "id": "2yQ2SOuBAr36"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sf7N87EBNv9r"
      },
      "outputs": [],
      "source": [
        "lda.perplexity(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK_Fv9_pNv9r"
      },
      "source": [
        "## ハイパーパラメータのチューニング\n",
        "* perplexityの値が最小になるようにチューニングする。\n",
        "  * トピック数(`n_components`)は、自分の都合で決めても良いかも。\n",
        "* トピック数に合わせて、`doc_topic_prior`と`topic_word_prior`の両方をチューニングする。\n",
        "  * トピック数が変わると、最も良い`doc_topic_prior`と`topic_word_prior`の値も、変わる。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6Tg509-Nv9s"
      },
      "source": [
        "### 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8Pr2OvYNv9s"
      },
      "outputs": [],
      "source": [
        "for n_components in [20, 30, 40, 50]:\n",
        "  for doc_topic_prior in [0.2, 0.1, 0.05]:\n",
        "    for topic_word_prior in [0.05, 0.02, 0.01]:\n",
        "      lda = LatentDirichletAllocation(\n",
        "        n_components=n_components,\n",
        "        doc_topic_prior=doc_topic_prior,\n",
        "        topic_word_prior=topic_word_prior,\n",
        "        max_iter=20,\n",
        "        evaluate_every=1,\n",
        "        verbose=1,\n",
        "        random_state=123,\n",
        "      )\n",
        "      lda.fit(X_train)\n",
        "      print(f\"-- test perplexity: {lda.perplexity(X_test):.2f}\")\n",
        "      print(f\"---- {n_components} topics, alpha={doc_topic_prior:.4f}, eta={topic_word_prior:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Lmrt5iXNv9s"
      },
      "source": [
        "### 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNrCAd2RNv9s"
      },
      "outputs": [],
      "source": [
        "for n_components in [20, 30, 40]:\n",
        "  for doc_topic_prior in [0.4, 0.3, 0.2]:\n",
        "    for topic_word_prior in [0.01, 0.005, 0.002]:\n",
        "      lda = LatentDirichletAllocation(\n",
        "        n_components=n_components,\n",
        "        doc_topic_prior=doc_topic_prior,\n",
        "        topic_word_prior=topic_word_prior,\n",
        "        max_iter=20,\n",
        "        evaluate_every=1,\n",
        "        verbose=1,\n",
        "        random_state=123,\n",
        "      )\n",
        "      lda.fit(X_train)\n",
        "      print(f\"-- test perplexity: {lda.perplexity(X_test):.2f}\")\n",
        "      print(f\"---- {n_components} topics, alpha={doc_topic_prior:.4f}, eta={topic_word_prior:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZBmH7aGNv9s"
      },
      "source": [
        "### 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jOpmLdO1Nv9s"
      },
      "outputs": [],
      "source": [
        "for n_components in [15, 20, 25]:\n",
        "  for doc_topic_prior in [0.6, 0.5, 0.4]:\n",
        "    for topic_word_prior in [0.03, 0.02, 0.01]:\n",
        "      lda = LatentDirichletAllocation(\n",
        "        n_components=n_components,\n",
        "        doc_topic_prior=doc_topic_prior,\n",
        "        topic_word_prior=topic_word_prior,\n",
        "        max_iter=20,\n",
        "        evaluate_every=1,\n",
        "        verbose=1,\n",
        "        random_state=123,\n",
        "      )\n",
        "      lda.fit(X_train)\n",
        "      print(f\"-- test perplexity: {lda.perplexity(X_test):.2f}\")\n",
        "      print(f\"---- {n_components} topics, alpha={doc_topic_prior:.4f}, eta={topic_word_prior:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPnF2AqFNv9s"
      },
      "source": [
        "### 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2IzFjiQNv9s"
      },
      "outputs": [],
      "source": [
        "for n_components in [10, 15, 20]:\n",
        "  for doc_topic_prior in [0.8, 0.7, 0.6]:\n",
        "    for topic_word_prior in [0.03, 0.02, 0.01]:\n",
        "      lda = LatentDirichletAllocation(\n",
        "        n_components=n_components,\n",
        "        doc_topic_prior=doc_topic_prior,\n",
        "        topic_word_prior=topic_word_prior,\n",
        "        max_iter=20,\n",
        "        evaluate_every=1,\n",
        "        verbose=1,\n",
        "        random_state=123,\n",
        "      )\n",
        "      lda.fit(X_train)\n",
        "      print(f\"-- test perplexity: {lda.perplexity(X_test):.2f}\")\n",
        "      print(f\"---- {n_components} topics, alpha={doc_topic_prior:.4f}, eta={topic_word_prior:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G37inpUqNv9s"
      },
      "source": [
        "## 最も良かった設定で改めて変分推論を実行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIU_puXWNv9s"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(stop_words=\"english\", min_df=20, max_df=0.5)\n",
        "X = vectorizer.fit_transform(ds[\"train\"][\"title\"] + ds[\"test\"][\"title\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmSETnCzNv9s"
      },
      "outputs": [],
      "source": [
        "lda = LatentDirichletAllocation(\n",
        "  n_components=15,\n",
        "  doc_topic_prior=0.6,\n",
        "  topic_word_prior=0.02,\n",
        "  max_iter=50,\n",
        "  evaluate_every=1,\n",
        "  verbose=1,\n",
        "  random_state=123,\n",
        ")\n",
        "lda.fit(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* モデルを保存"
      ],
      "metadata": {
        "id": "mLNTEzdJOCS3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDPziS_3Nv9s"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "outfile = \"lda_model.pk\"\n",
        "with open(outfile, 'wb') as pickle_file:\n",
        "  pickle.dump(lda, pickle_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9Y82I77Nv9s"
      },
      "source": [
        "## 可視化"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* LDAの学習時と語彙が同じになるようにする。"
      ],
      "metadata": {
        "id": "2dAXLjl9BemD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIL_sSsyNv9t"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "ds = load_dataset(\"CShorten/ML-ArXiv-Papers\")\n",
        "ds = ds[\"train\"].train_test_split(test_size=0.1, seed=1234)\n",
        "vectorizer = CountVectorizer(stop_words=\"english\", min_df=20, max_df=0.5)\n",
        "vectorizer.fit(ds[\"train\"][\"title\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* データセット全体でterm frequencyを計算しなおす。"
      ],
      "metadata": {
        "id": "vSfwOcv1BSlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"CShorten/ML-ArXiv-Papers\")\n",
        "X = vectorizer.transform(ds[\"train\"][\"title\"])"
      ],
      "metadata": {
        "id": "piOZN7lHBFKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 語彙サイズがLDAの学習時と同じであることを確認する。"
      ],
      "metadata": {
        "id": "22YjeZ-7Bonw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "id": "HXht4CMVBN5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbATHJeDNv9t"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "outfile = \"lda_model.pk\"\n",
        "with open(outfile, \"rb\") as pickle_file:\n",
        "  lda = pickle.load(pickle_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* pyLDAvisはあらかじめインストールしておく。"
      ],
      "metadata": {
        "id": "hzqtRHVtOEUR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHaXwnCuNv9t"
      },
      "outputs": [],
      "source": [
        "import pyLDAvis\n",
        "import pyLDAvis.lda_model\n",
        "\n",
        "pyLDAvis.enable_notebook()\n",
        "pyLDAvis.lda_model.prepare(lda, X, vectorizer, mds='mmds')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpGMMnK0Nv9t"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}